{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9uihVNiQn73"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o76DP8wsQn77"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "#import itertools as it\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4erwJ19Qn7-"
      },
      "source": [
        "# Neurons as Logic Gates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plaVejTnQn7_"
      },
      "source": [
        "As an introduction to neural networks and their component neurons, we are going to look at using neurons to implement the most primitive logic computations:  logic gates.  Let's go!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocxQcNDzQn7_"
      },
      "source": [
        "##### The Sigmoid Function\n",
        "\n",
        "The basic, classic activation function that we apply to neurons is a  sigmoid (sometimes just called *the* sigmoid function) function:  the standard logistic function.\n",
        "\n",
        "$$\n",
        "\\sigma = \\frac{1}{1 + e^{-x}}\n",
        "$$\n",
        "\n",
        "$\\sigma$ ranges from (0, 1). When the input $x$ is negative, $\\sigma$ is close to 0. When $x$ is positive, $\\sigma$ is close to 1. At $x=0$, $\\sigma=0.5$\n",
        "\n",
        "We can implement this conveniently with NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQNd_84kQn8A"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid function\"\"\"\n",
        "    return 1.0 / (1.0 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J63is0bpQn8B"
      },
      "source": [
        "And plot it with matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "7gcLv1dcQn8B",
        "outputId": "b77f701a-04a8-41c7-dd0e-5ed28d2175a9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot The sigmoid function\n",
        "xs = np.linspace(-10, 10, num=100, dtype=np.float32)\n",
        "activation = sigmoid(xs)\n",
        "\n",
        "fig = plt.figure(figsize=(4,3))\n",
        "plt.plot(xs, activation)\n",
        "plt.plot(0,.5,'ro')\n",
        "\n",
        "plt.grid(True, which='both')\n",
        "plt.axhline(y=0, color='y')\n",
        "plt.axvline(x=0, color='y')\n",
        "plt.ylim([-0.1, 1.15])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wruA1HgVmi3B",
        "outputId": "2958638d-a906-47ef-a867-50bb7886b68c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-10.          -9.797979    -9.59596     -9.393939    -9.191919\n",
            "  -8.989899    -8.787879    -8.585858    -8.383839    -8.181818\n",
            "  -7.979798    -7.7777777   -7.5757575   -7.3737373   -7.171717\n",
            "  -6.969697    -6.767677    -6.5656567   -6.3636365   -6.1616163\n",
            "  -5.959596    -5.757576    -5.5555553   -5.353535    -5.151515\n",
            "  -4.949495    -4.7474747   -4.5454545   -4.3434343   -4.141414\n",
            "  -3.939394    -3.7373738   -3.5353534   -3.3333333   -3.131313\n",
            "  -2.929293    -2.7272727   -2.5252526   -2.3232324   -2.121212\n",
            "  -1.919192    -1.7171717   -1.5151515   -1.3131313   -1.1111112\n",
            "  -0.90909094  -0.7070707   -0.5050505   -0.3030303   -0.1010101\n",
            "   0.1010101    0.3030303    0.5050505    0.7070707    0.90909094\n",
            "   1.1111112    1.3131313    1.5151515    1.7171717    1.919192\n",
            "   2.121212     2.3232324    2.5252526    2.7272727    2.929293\n",
            "   3.131313     3.3333333    3.5353534    3.7373738    3.939394\n",
            "   4.141414     4.3434343    4.5454545    4.7474747    4.949495\n",
            "   5.151515     5.353535     5.5555553    5.757576     5.959596\n",
            "   6.1616163    6.3636365    6.5656567    6.767677     6.969697\n",
            "   7.171717     7.3737373    7.5757575    7.7777777    7.979798\n",
            "   8.181818     8.383839     8.585858     8.787879     8.989899\n",
            "   9.191919     9.393939     9.59596      9.797979    10.        ]\n"
          ]
        }
      ],
      "source": [
        "xs = np.linspace(-10, 10, num=100, dtype=np.float32)\n",
        "print(xs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBf0C3JdQn8C"
      },
      "source": [
        "## An Example with OR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjWqK41AQn8C"
      },
      "source": [
        "##### OR Logic\n",
        "A logic gate takes in two boolean (true/false or 1/0) inputs, and returns either a 0 or 1 depending on its rule. The truth table for a logic gate shows the outputs for each combination of inputs: (0, 0), (0, 1), (1,0), and (1, 1). For example, let's look at the truth table for an Or-gate:\n",
        "\n",
        "<table>\n",
        "<tr><th colspan=\"3\">OR gate truth table</th></tr>\n",
        "<tr><th colspan=\"2\">Input</th><th>Output</th></tr>\n",
        "<tr><td>0</td><td>0</td><td>0</td></tr>\n",
        "<tr><td>0</td><td>1</td><td>1</td></tr>\n",
        "<tr><td>1</td><td>0</td><td>1</td></tr>\n",
        "<tr><td>1</td><td>1</td><td>1</td></tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSx82PbQQn8C"
      },
      "source": [
        "##### OR as a Neuron\n",
        "\n",
        "A neuron that uses the sigmoid activation function outputs a value between (0, 1). This naturally leads us to think about boolean values. Imagine a neuron that takes in two inputs, $x_1$ and $x_2$, and a bias term:\n",
        "\n",
        "   <img src=\"logic01.png\" width=50% />\n",
        "\n",
        "By limiting the inputs of $x_1$ and $x_2$ to be in $\\left\\{0, 1\\right\\}$, we can simulate the effect of logic gates with our neuron. The goal is to find the weights (represented by ? marks above), such that it returns an output close to 0 or 1 depending on the inputs.  What weights should we use to output the same results as OR? Remember: $\\sigma(z)$ is close to 0 when $z$ is largely negative (around -10 or less), and is close to 1 when $z$ is largely positive (around +10 or greater).\n",
        "\n",
        "$$\n",
        "z = w_1 x_1 + w_2 x_2 + b\n",
        "$$\n",
        "\n",
        "Let's think this through:\n",
        "\n",
        "* When $x_1$ and $x_2$ are both 0, the only value affecting $z$ is $b$. Because we want the result for input (0, 0) to be close to zero, $b$ should be negative (at least -10) to get the very left-hand part of the sigmoid.\n",
        "* If either $x_1$ or $x_2$ is 1, we want the output to be close to 1. That means the weights associated with $x_1$ and $x_2$ should be enough to offset $b$ to the point of causing $z$ to be at least 10 (i.e., to the far right part of the sigmoid).\n",
        "\n",
        "Let's give $b$ a value of -10. How big do we need $w_1$ and $w_2$ to be?  At least +20 will get us to +10 for just one of $\\{w_1, w_2\\}$ being on.\n",
        "\n",
        "So let's try out $w_1=20$, $w_2=20$, and $b=-10$:\n",
        "\n",
        " <img src=\"logic02.png\" width=50% />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g054XD4eQn8D"
      },
      "source": [
        "##### Some Utility Functions\n",
        "Since we're going to be making several example logic gates (from different sets of weights and biases), here are two helpers.  The first takes our weights and baises and turns them into a two-argument function that we can use like `and(a,b)`.  The second is for printing a truth table for a gate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wleq8ry3zEAV"
      },
      "outputs": [],
      "source": [
        "# Logic gate function\n",
        "def logic_gate(w1, w2, b, x1, x2):\n",
        "  ''' logic_gate is a function which returns the results of\n",
        "        taking two args and  (hopefully) acts like a logic gate (and/or/not/etc.).\n",
        "        its behavior is determined by w1,w2,b. '''\n",
        "  return sigmoid(w1 * x1 + w2 * x2 + b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhpF7mgMQn8D"
      },
      "outputs": [],
      "source": [
        "# Test function that takes a function with two arguments\n",
        "def test_gate(w1, w2, b):\n",
        "  for x1 in range(2):\n",
        "    for x2 in range(2):\n",
        "      print(\"{}, {}: {}\".format(x1, x2, np.round(logic_gate(w1, w2, b, x1, x2))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJxJ1-q6Qn8D"
      },
      "source": [
        "Let's see how we did.  Here's the gold-standard truth table.\n",
        "\n",
        "<table>\n",
        "<tr><th colspan=\"3\">OR gate truth table</th></tr>\n",
        "<tr><th colspan=\"2\">Input</th><th>Output</th></tr>\n",
        "<tr><td>0</td><td>0</td><td>0</td></tr>\n",
        "<tr><td>0</td><td>1</td><td>1</td></tr>\n",
        "<tr><td>1</td><td>0</td><td>1</td></tr>\n",
        "<tr><td>1</td><td>1</td><td>1</td></tr>\n",
        "</table>\n",
        "\n",
        "And our result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gVu1GrK1-EN",
        "outputId": "1cb53377-1b24-4d4c-ea0c-b2ec4b8c3fbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0, 0: 0.0\n",
            "0, 1: 1.0\n",
            "1, 0: 1.0\n",
            "1, 1: 1.0\n"
          ]
        }
      ],
      "source": [
        "or_gate = test_gate(20, 20, -10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qci7rZBQQn8D"
      },
      "source": [
        "This matches - great!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ycn5zQTQn8E"
      },
      "source": [
        "# Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWh053OQQn8E"
      },
      "source": [
        "##### Part 1:  AND Gate\n",
        "\n",
        "Now you try finding the appropriate weight values for each truth table. Try not to guess and check. Think through it logically and try to derive values that work.\n",
        "\n",
        "<table>\n",
        "<tr><th colspan=\"3\">AND gate truth table</th></tr>\n",
        "<tr><th colspan=\"2\">Input</th><th>Output</th></tr>\n",
        "<tr><td>0</td><td>0</td><td>0</td></tr>\n",
        "<tr><td>0</td><td>1</td><td>0</td></tr>\n",
        "<tr><td>1</td><td>0</td><td>0</td></tr>\n",
        "<tr><td>1</td><td>1</td><td>1</td></tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "7ARyNpeqQn8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3b5539-d86a-457c-8698-64388880bca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0, 0: 0.0\n",
            "0, 1: 0.0\n",
            "1, 0: 0.0\n",
            "1, 1: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Fill in the w1, w2, and b parameters such that the truth table matches\n",
        "and_gate = test_gate(10, 10, -15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ_MIZqVQn8E"
      },
      "source": [
        "##### Part 2: NOR (Not Or) Gate\n",
        "<table>\n",
        "<tr><th colspan=\"3\">NOR gate truth table</th></tr>\n",
        "<tr><th colspan=\"2\">Input</th><th>Output</th></tr>\n",
        "<tr><td>0</td><td>0</td><td>1</td></tr>\n",
        "<tr><td>0</td><td>1</td><td>0</td></tr>\n",
        "<tr><td>1</td><td>0</td><td>0</td></tr>\n",
        "<tr><td>1</td><td>1</td><td>0</td></tr>\n",
        "</table>\n",
        "<table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "V4Bir8bdQn8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a7d7619-ee99-439e-a206-db22519776c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0, 0: 1.0\n",
            "0, 1: 0.0\n",
            "1, 0: 0.0\n",
            "1, 1: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Fill in the w1, w2, and b parameters such that the truth table matches\n",
        "nor_gate = test_gate(-20, -20, 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kouMg6SiQn8F"
      },
      "source": [
        "##### Part 3: NAND (Not And) Gate\n",
        "<table>\n",
        "<tr><th colspan=\"3\">NAND gate truth table</th></tr>\n",
        "<tr><th colspan=\"2\">Input</th><th>Output</th></tr>\n",
        "<tr><td>0</td><td>0</td><td>1</td></tr>\n",
        "<tr><td>0</td><td>1</td><td>1</td></tr>\n",
        "<tr><td>1</td><td>0</td><td>1</td></tr>\n",
        "<tr><td>1</td><td>1</td><td>0</td></tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rRC94EkhQn8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1699ee3-ad8a-4659-f60b-c1078e9d55ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0, 0: 1.0\n",
            "0, 1: 1.0\n",
            "1, 0: 1.0\n",
            "1, 1: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Fill in the w1, w2, and b parameters such that the truth table matches\n",
        "nand_gate = test_gate(-10, -10, 15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I029hJsQn8F"
      },
      "source": [
        "## Solutions 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tm-zb34Qn8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55331a1-1745-4ecd-c25d-604d00894740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method BaseResourceVariable.numpy of <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>>\n"
          ]
        }
      ],
      "source": [
        "b = tf.Variable(tf.zeros([1]))\n",
        "print(b.numpy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEeKlwb7Qn8G"
      },
      "source": [
        "# Learning a Logic Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROzXwgizQn8G"
      },
      "source": [
        "We can use TensorFlow to try and teach a model to learn the correct weights and bias by passing in our truth table as training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgSRfaq-6ryc"
      },
      "outputs": [],
      "source": [
        "# Define LogicGate Model using tf.Module\n",
        "class LogicGate(tf.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.built = False  # Track if model is initialized\n",
        "\n",
        "    def __call__(self, x, train=True):\n",
        "        # Initialize weights and bias on first call\n",
        "        if not self.built:\n",
        "            input_dim = x.shape[-1]  # Number of input features\n",
        "            self.w = tf.Variable(tf.random.normal([input_dim, 1]), name=\"weights\")\n",
        "            self.b = tf.Variable(tf.zeros([1]), name=\"bias\")\n",
        "            self.built = True\n",
        "\n",
        "        # Compute logits: z = Wx + b\n",
        "        z = tf.add(tf.matmul(x, self.w), self.b)\n",
        "        return tf.sigmoid(z)  # Apply sigmoid\n",
        "\n",
        "# Loss function (Mean Squared Error)\n",
        "def compute_loss(y_pred, y_true):\n",
        "    return tf.reduce_mean(tf.square(y_pred - y_true))\n",
        "\n",
        "# Training function\n",
        "def train_model(model, x_train, y_train, learning_rate=0.5, epochs=5000):\n",
        "  # Iterate over the training data\n",
        "  for epoch in range(epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_pred = model(x_train)  # Forward pass\n",
        "      loss = compute_loss(y_pred, y_train)\n",
        "\n",
        "    # Update the parameters with respect to the gradient calculations\n",
        "    grads = tape.gradient(loss, model.variables)\n",
        "    for g,v in zip(grads, model.variables):\n",
        "      v.assign_sub(learning_rate * g)\n",
        "\n",
        "    # Print progress every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "      acc = compute_accuracy(model, x_train, y_train)\n",
        "      print(f\"Epoch {epoch}, Loss: {loss.numpy():.4f}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Accuracy function\n",
        "def compute_accuracy(model, x, y_true):\n",
        "    y_pred = model(x, train=False)\n",
        "    y_pred_rounded = tf.round(y_pred)\n",
        "    correct = tf.equal(y_pred_rounded, y_true)\n",
        "    return tf.reduce_mean(tf.cast(correct, tf.float32)).numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSSTaO3y6w-O",
        "outputId": "12cb359d-a1f7-4496-a424-525100b9b114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.2606, Accuracy: 0.5000\n",
            "Epoch 1000, Loss: 0.0124, Accuracy: 1.0000\n",
            "Epoch 2000, Loss: 0.0057, Accuracy: 1.0000\n",
            "Epoch 3000, Loss: 0.0036, Accuracy: 1.0000\n",
            "Epoch 4000, Loss: 0.0026, Accuracy: 1.0000\n",
            "\n",
            "Learned weight for w1: 5.736032962799072\n",
            "Learned weight for w2: 5.736032962799072\n",
            "Learned bias: -8.69567584991455\n",
            "\n",
            "Predicted Truth Table:\n",
            "[[0. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 1. 1.]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Prepare AND gate dataset\n",
        "and_table = np.array([[0, 0, 0],\n",
        "                      [1, 0, 0],\n",
        "                      [0, 1, 0],\n",
        "                      [1, 1, 1]], dtype=np.float32)\n",
        "\n",
        "x_train = and_table[:, :2]  # Inputs: x1, x2\n",
        "y_train = and_table[:, 2:]  # Labels: y\n",
        "\n",
        "# Initialize and train model\n",
        "model = LogicGate()\n",
        "train_model(model, x_train, y_train)\n",
        "\n",
        "# Evaluate and print results\n",
        "w1, w2 = model.w.numpy().flatten()\n",
        "b = model.b.numpy().flatten()[0]\n",
        "print(f\"\\nLearned weight for w1: {w1}\")\n",
        "print(f\"Learned weight for w2: {w2}\")\n",
        "print(f\"Learned bias: {b}\\n\")\n",
        "\n",
        "# Test model predictions\n",
        "y_pred = model(x_train, train=False).numpy().round().astype(np.uint8)\n",
        "print(\"Predicted Truth Table:\")\n",
        "print(np.column_stack((and_table[:, :2], y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tenXRFveQn8F"
      },
      "source": [
        "# Limits of Single Neurons\n",
        "\n",
        "If you've taken computer science courses, you may know that the XOR gates are the basis of computation. They can be used as half-adders, the foundation of being able to add numbers together. Here's the truth table for XOR:\n",
        "\n",
        "##### XOR (Exclusive Or) Gate\n",
        "<table>\n",
        "<tr><th colspan=\"3\">NAND gate truth table</th></tr>\n",
        "<tr><th colspan=\"2\">Input</th><th>Output</th></tr>\n",
        "<tr><td>0</td><td>0</td><td>0</td></tr>\n",
        "<tr><td>0</td><td>1</td><td>1</td></tr>\n",
        "<tr><td>1</td><td>0</td><td>1</td></tr>\n",
        "<tr><td>1</td><td>1</td><td>0</td></tr>\n",
        "</table>\n",
        "\n",
        "Now the question is, can you create a set of weights such that a single neuron can output this property?  It turns out that you cannot. Single neurons can't correlate inputs, so it's just confused. So individual neurons are out. Can we still use neurons to somehow form an XOR gate?\n",
        "\n",
        "What if we tried something more complex:\n",
        "\n",
        " <img src=\"logic03.png\" width=50% />\n",
        "\n",
        "Here, we've got the inputs going to two separate gates: the top neuron is an OR gate, and the bottom is a NAND gate. The output of these gates is passed to another neuron, which is an AND gate. If you work out the outputs at each combination of input values, you'll see that this is an XOR gate!\n",
        "\n",
        "XOR(A,B)=OR(A, B) AND NAND(A,B)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THXGPuqVQn8G"
      },
      "outputs": [],
      "source": [
        "# Make sure you have or_gate, nand_gate, and and_gate working from above\n",
        "#def xor_gate(a, b):\n",
        "#    c = or_gate(a, b)\n",
        "#    d = nand_gate(a, b)\n",
        "#    return and_gate(c, d)\n",
        "#test(xor_gate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmyjlAWXQn8G"
      },
      "source": [
        "Thus, we can see how chaining together neurons can compose more complex models than we'd otherwise have access to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFVMtbjYQn8J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LawaqclfQn8K"
      },
      "source": [
        "# Exercise 2: Learning an XOR Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgssswnVQn8K"
      },
      "source": [
        "<img src=\"logic03.png\" width=50% />"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define XOR Logic Model using tf.Module\n",
        "class XOR_Model(tf.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.initialized = False\n",
        "\n",
        "    def __call__(self, inputs, training=True):\n",
        "        if not self.initialized:\n",
        "            input_size = inputs.shape[-1]\n",
        "            self.W1= tf.Variable(tf.random.normal([input_size, 2]), name=\"W1\")\n",
        "            self.b1 = tf.Variable(tf.zeros([2]), name=\"b1\")\n",
        "\n",
        "            self.W2 = tf.Variable(tf.random.normal([2, 1]), name=\"W2\")\n",
        "            self.b2 = tf.Variable(tf.zeros([1]), name=\"b2\")\n",
        "\n",
        "            self.initialized = True\n",
        "\n",
        "        hidden_layer = tf.sigmoid(tf.add(tf.matmul(inputs, self.W1), self.b1))\n",
        "        output_layer = tf.add(tf.matmul(hidden_layer, self.W2), self.b2)\n",
        "        return tf.sigmoid(output_layer)\n",
        "\n",
        "# Loss function\n",
        "def calculate_loss(predicted, actual):\n",
        "    return tf.reduce_mean(tf.square(predicted - actual))\n",
        "\n",
        "# Training function\n",
        "def train_xor_model(model, x_data, y_data, learning_rate=0.3, epochs=15000):\n",
        "    for epoch in range(epochs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(x_data)\n",
        "            loss = calculate_loss(predictions, y_data)\n",
        "\n",
        "        gradients = tape.gradient(loss, model.variables)\n",
        "        for grad, var in zip(gradients, model.variables):\n",
        "            var.assign_sub(learning_rate * grad)\n",
        "\n",
        "        if epoch % 2000 == 0:\n",
        "            accuracy = evaluate_accuracy(model, x_data, y_data)\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.numpy():.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Accuracy function\n",
        "def evaluate_accuracy(model, x_data, y_actual):\n",
        "    y_predicted = model(x_data, training=False)\n",
        "    rounded_predictions = tf.round(y_predicted)\n",
        "    correct_predictions = tf.equal(rounded_predictions, y_actual)\n",
        "    return tf.reduce_mean(tf.cast(correct_predictions, tf.float32)).numpy()\n",
        "\n",
        "# Prepare XOR dataset\n",
        "xor_data = np.array([[0, 0, 0],\n",
        "                      [1, 0, 1],\n",
        "                      [0, 1, 1],\n",
        "                      [1, 1, 0]], dtype=np.float32)\n",
        "\n",
        "x_values = xor_data[:, :2]\n",
        "y_values = xor_data[:, 2:]\n",
        "\n",
        "# Initialize and train the model\n",
        "xor_model = XOR_Model()\n",
        "train_xor_model(xor_model, x_values, y_values)\n",
        "\n",
        "print(\"\\nTrained Weights and Biases:\")\n",
        "print(f\"Hidden Weights:\\n{xor_model.W1.numpy()}\")\n",
        "print(f\"Hidden Bias: {xor_model.b1.numpy()}\")\n",
        "print(f\"Output Weights:\\n{xor_model.W2.numpy()}\")\n",
        "print(f\"Output Bias: {xor_model.b2.numpy()}\\n\")\n",
        "\n",
        "# Test model predictions\n",
        "y_predicted = xor_model(x_values, training=False).numpy().round().astype(np.uint8)\n",
        "print(\"Predicted XOR Truth Table:\")\n",
        "prediction_output = np.column_stack((xor_data[:, :2], y_predicted))\n",
        "print(prediction_output)\n",
        "\n",
        "print(\"\\nCorrect?\", np.allclose(xor_data, prediction_output))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F-a9Zxbm7gz",
        "outputId": "a0b0afb7-038c-46cb-9d1b-ca9abcf86140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.2581, Accuracy: 0.5000\n",
            "Epoch 2000, Loss: 0.2500, Accuracy: 0.2500\n",
            "Epoch 4000, Loss: 0.2495, Accuracy: 0.7500\n",
            "Epoch 6000, Loss: 0.1860, Accuracy: 0.7500\n",
            "Epoch 8000, Loss: 0.0350, Accuracy: 1.0000\n",
            "Epoch 10000, Loss: 0.0085, Accuracy: 1.0000\n",
            "Epoch 12000, Loss: 0.0044, Accuracy: 1.0000\n",
            "Epoch 14000, Loss: 0.0030, Accuracy: 1.0000\n",
            "\n",
            "Trained Weights and Biases:\n",
            "Hidden Weights:\n",
            "[[ 6.670654 -4.573268]\n",
            " [ 6.640257 -4.568026]]\n",
            "Hidden Bias: [-2.9272497  6.8351235]\n",
            "Output Weights:\n",
            "[[7.1517053]\n",
            " [7.3839283]]\n",
            "Output Bias: [-10.648183]\n",
            "\n",
            "Predicted XOR Truth Table:\n",
            "[[0. 0. 0.]\n",
            " [1. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 1. 0.]]\n",
            "\n",
            "Correct? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nKopifre0Rzb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}